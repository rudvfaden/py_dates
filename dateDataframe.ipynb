{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465c0171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "WARNING: package sun.security.action not in java.base\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/09 18:53:52 WARN Utils: Your hostname, MacBook-Air-tilhrende-Rud.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.154 instead (on interface en0)\n",
      "26/01/09 18:53:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/09 18:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARNING: A terminally deprecated method in sun.misc.Unsafe has been called\n",
      "WARNING: sun.misc.Unsafe::arrayBaseOffset has been called by org.apache.spark.unsafe.Platform (file:/Users/rudfaden/Documents/py_dates/env/lib/python3.14/site-packages/pyspark/jars/spark-unsafe_2.13-4.1.0.jar)\n",
      "WARNING: Please consider reporting this to the maintainers of class org.apache.spark.unsafe.Platform\n",
      "WARNING: sun.misc.Unsafe::arrayBaseOffset will be removed in a future release\n",
      "26/01/09 18:53:55 WARN FileSystem: Cannot load filesystem\n",
      "java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.viewfs.ViewFileSystem could not be instantiated\n",
      "\tat java.base/java.util.ServiceLoader.fail(ServiceLoader.java:552)\n",
      "\tat java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:712)\n",
      "\tat java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:672)\n",
      "\tat java.base/java.util.ServiceLoader$2.next(ServiceLoader.java:1256)\n",
      "\tat org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3525)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562)\n",
      "\tat org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:77)\n",
      "\tat org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:209)\n",
      "\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:208)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:56)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:176)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:176)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:175)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:187)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:185)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:182)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:198)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:198)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:565)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: java.lang.UnsupportedOperationException: getSubject is not supported\n",
      "\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n",
      "\tat org.apache.hadoop.fs.viewfs.ViewFileSystem.<init>(ViewFileSystem.java:281)\n",
      "\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\n",
      "\tat java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:707)\n",
      "\t... 28 more\n",
      "26/01/09 18:53:55 WARN SharedState: Cannot qualify the warehouse path, leaving it unqualified.\n",
      "java.lang.UnsupportedOperationException: getSubject is not supported\n",
      "\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3888)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3878)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3666)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:302)\n",
      "\tat org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:82)\n",
      "\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:81)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:176)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:176)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:175)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:187)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:185)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:182)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:198)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:198)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:565)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "26/01/09 18:53:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "|      Date|  DateID|Year|Quarter|Month|Day|Week|QuarterNameLong|QuarterNameShort|QuarterNumberString|MonthNameLong|MonthNameShort|MonthNumberString|DayNumberString|WeekNameLong|WeekNameShort|WeekNumberString|DayOfWeek|YearMonthString|DayOfWeekNameLong|DayOfWeekNameShort|DayOfMonth|DayOfYear|MonthNameLong_DK|MonthNameShort_DK|DayofWeekNameLong_DK|IsHoliday|IsToady|HolidayName|\n",
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "|2024-01-01|20240101|2024|      1|    1|  1|   1|    1st quarter|              Q1|                 01|      January|           Jan|               01|             01|      week01|          w01|              01|        2|        2024/01|           Monday|               Mon|         1|        1|          Januar|              Jan|              Mandag|        1|      0|  Nytårsdag|\n",
      "|2024-01-02|20240102|2024|      1|    1|  2|   1|    1st quarter|              Q1|                 01|      January|           Jan|               01|             02|      week01|          w01|              01|        3|        2024/01|          Tuesday|               Tue|         2|        2|          Januar|              Jan|             Tirsdag|        0|      0|       NULL|\n",
      "|2024-01-03|20240103|2024|      1|    1|  3|   1|    1st quarter|              Q1|                 01|      January|           Jan|               01|             03|      week01|          w01|              01|        4|        2024/01|        Wednesday|               Wed|         3|        3|          Januar|              Jan|              Onsdag|        0|      0|       NULL|\n",
      "|2024-01-04|20240104|2024|      1|    1|  4|   1|    1st quarter|              Q1|                 01|      January|           Jan|               01|             04|      week01|          w01|              01|        5|        2024/01|         Thursday|               Thu|         4|        4|          Januar|              Jan|             Torsdag|        0|      0|       NULL|\n",
      "|2024-01-05|20240105|2024|      1|    1|  5|   1|    1st quarter|              Q1|                 01|      January|           Jan|               01|             05|      week01|          w01|              01|        6|        2024/01|           Friday|               Fri|         5|        5|          Januar|              Jan|              Fredag|        0|      0|       NULL|\n",
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "+----------+---------+--------------------+--------------------+\n",
      "|      Date|IsHoliday|DayOfWeekNameLong_DK|         HolidayName|\n",
      "+----------+---------+--------------------+--------------------+\n",
      "|2025-01-01|        1|              Onsdag|           Nytårsdag|\n",
      "|2025-04-17|        1|             Torsdag|         Skærtorsdag|\n",
      "|2025-04-18|        1|              Fredag|          Langfredag|\n",
      "|2025-04-21|        1|              Mandag|         2. Påskedag|\n",
      "|2025-05-29|        1|             Torsdag|Kristi himmelfart...|\n",
      "|2025-05-30|        1|              Fredag|Fredag efter Kris...|\n",
      "|2025-06-05|        1|             Torsdag|        Grundlovsdag|\n",
      "|2025-06-09|        1|              Mandag|         2. Pinsedag|\n",
      "|2025-12-24|        1|              Onsdag|       Juleaftensdag|\n",
      "|2025-12-25|        1|             Torsdag|             Juledag|\n",
      "|2025-12-26|        1|              Fredag|          2. Juledag|\n",
      "|2025-12-31|        1|              Onsdag|      Nytåraftensdag|\n",
      "+----------+---------+--------------------+--------------------+\n",
      "\n",
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "|      Date|  DateID|Year|Quarter|Month|Day|Week|QuarterNameLong|QuarterNameShort|QuarterNumberString|MonthNameLong|MonthNameShort|MonthNumberString|DayNumberString|WeekNameLong|WeekNameShort|WeekNumberString|DayOfWeek|YearMonthString|DayOfWeekNameLong|DayOfWeekNameShort|DayOfMonth|DayOfYear|MonthNameLong_DK|MonthNameShort_DK|DayofWeekNameLong_DK|IsHoliday|IsToady|HolidayName|\n",
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "|2026-01-09|20260109|2026|      1|    1|  9|   2|    1st quarter|              Q1|                 01|      January|           Jan|               01|             09|      week02|          w02|              02|        6|        2026/01|           Friday|               Fri|         9|        9|          Januar|              Jan|              Fredag|        0|      1|       NULL|\n",
      "+----------+--------+----+-------+-----+---+----+---------------+----------------+-------------------+-------------+--------------+-----------------+---------------+------------+-------------+----------------+---------+---------------+-----------------+------------------+----------+---------+----------------+-----------------+--------------------+---------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from danish_banking_holidays.calendar import DanishBankingCalendar\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DanishBankingHolidayApp\").getOrCreate() # type: ignore\n",
    "# Create a UDF that wraps the calendar.is_holiday method\n",
    "\n",
    "# Initialize the calendar - REQUIRED!\n",
    "calendar = DanishBankingCalendar()\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_danish_holiday(date):\n",
    "    return calendar.is_holiday(date)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def holiday_name(date):\n",
    "    calendar_dict = calendar.get_holiday_name(date)\n",
    "    if not calendar_dict:\n",
    "        return None\n",
    "    return calendar_dict.get(date)\n",
    "\n",
    "# Register the UDF so it can be used in SQL expressions\n",
    "spark.udf.register(\"is_danish_holiday\", is_danish_holiday)\n",
    "spark.udf.register(\"holiday_name\", holiday_name)\n",
    "\n",
    "\n",
    "# define boundaries\n",
    "startdate = (datetime.now() + relativedelta(years=-2)).replace(month=1, day=1) \n",
    "enddate   = (datetime.now() + relativedelta(years=2)).replace(month=12, day=31)  # datetime.strptime('2023-10-01','%Y-%m-%d')\n",
    "\n",
    "\n",
    "# define column names and its transformation rules on the Date column\n",
    "column_rule_df = spark.createDataFrame([\n",
    "    (\"DateID\",              \"cast(date_format(date, 'yyyyMMdd') as int)\"),     # 20230101\n",
    "    (\"Year\",                \"year(date)\"),                                     # 2023\n",
    "    (\"Quarter\",             \"quarter(date)\"),                                  # 1\n",
    "    (\"Month\",               \"month(date)\"),                                    # 1\n",
    "    (\"Day\",                 \"day(date)\"),                                      # 1\n",
    "    (\"Week\",                \"weekofyear(date)\"),                               # 1\n",
    "    (\"QuarterNameLong\",     \"date_format(date, 'QQQQ')\"),                      # 1st qaurter\n",
    "    (\"QuarterNameShort\",    \"date_format(date, 'QQQ')\"),                       # Q1\n",
    "    (\"QuarterNumberString\", \"date_format(date, 'QQ')\"),                        # 01\n",
    "    (\"MonthNameLong\",       \"date_format(date, 'MMMM')\"),                      # January\n",
    "    (\"MonthNameShort\",      \"date_format(date, 'MMM')\"),                       # Jan\n",
    "    (\"MonthNumberString\",   \"date_format(date, 'MM')\"),                        # 01\n",
    "    (\"DayNumberString\",     \"date_format(date, 'dd')\"),                        # 01\n",
    "    (\"WeekNameLong\",        \"concat('week', lpad(weekofyear(date), 2, '0'))\"), # week 01\n",
    "    (\"WeekNameShort\",       \"concat('w', lpad(weekofyear(date), 2, '0'))\"),    # w01\n",
    "    (\"WeekNumberString\",    \"lpad(weekofyear(date), 2, '0')\"),                 # 01\n",
    "    (\"DayOfWeek\",           \"dayofweek(date)\"),                                # 1\n",
    "    (\"YearMonthString\",     \"date_format(date, 'yyyy/MM')\"),                   # 2023/01\n",
    "    (\"DayOfWeekNameLong\",   \"date_format(date, 'EEEE')\"),                      # Sunday\n",
    "    (\"DayOfWeekNameShort\",  \"date_format(date, 'EEE')\"),                       # Sun\n",
    "    (\"DayOfMonth\",          \"cast(date_format(date, 'd') as int)\"),            # 1\n",
    "    (\"DayOfYear\",           \"cast(date_format(date, 'D') as int)\"),            # 1\n",
    "    (\"MonthNameLong_DK\",   \"CASE WHEN month(date) = 1 THEN 'Januar' WHEN month(date) = 2 THEN 'Februar' WHEN month(date) = 3 THEN 'Marts' WHEN month(date) = 4 THEN 'April' WHEN month(date) = 5 THEN 'Maj' WHEN month(date) = 6 THEN 'Juni' WHEN month(date) = 7 THEN 'Juli' WHEN month(date) = 8 THEN 'August' WHEN month(date) = 9 THEN 'September' WHEN month(date) = 10 THEN 'Oktober' WHEN month(date) = 11 THEN 'November' WHEN month(date) = 12 THEN 'December' ELSE '' END\"), # Januar\n",
    "    (\"MonthNameShort_DK\",  \"CASE WHEN month(date) = 1 THEN 'Jan' WHEN month(date) = 2 THEN 'Feb' WHEN month(date) = 3 THEN 'Mar' WHEN month(date) = 4 THEN 'Apr' WHEN month(date) = 5 THEN 'Maj' WHEN month(date) = 6 THEN 'Jun' WHEN month(date) = 7 THEN 'Jul' WHEN month(date) = 8 THEN 'Aug' WHEN month(date) = 9 THEN 'Sep' WHEN month(date) = 10 THEN 'Okt' WHEN month(date) = 11 THEN 'Nov' WHEN month(date) = 12 THEN 'Dec' ELSE '' END\"),                                    # Jan\n",
    "    (\"DayofWeekNameLong_DK\", \"CASE WHEN dayofweek(date) = 1 THEN 'Søndag' WHEN dayofweek(date) = 2 THEN 'Mandag' WHEN dayofweek(date) = 3 THEN 'Tirsdag' WHEN dayofweek(date) = 4 THEN 'Onsdag' WHEN dayofweek(date) = 5 THEN 'Torsdag' WHEN dayofweek(date) = 6 THEN 'Fredag' WHEN dayofweek(date) = 7 THEN 'Lørdag' ELSE '' END\") ,                                                                                                                                         # Sonntag\n",
    "    (\"IsHoliday\", \"cast(is_danish_holiday(date) as int)\"),  # 1 if holiday else 0\n",
    "    (\"IsToady\",   \"CASE WHEN date = current_date() THEN 1 ELSE 0 END\") , # 1 if today else 0\n",
    "    (\"HolidayName\", \"holiday_name(date)\")  # name of the holiday or null\n",
    "], [\"new_column_name\", \"expression\"])\n",
    "\n",
    "\n",
    "\n",
    "# explode dates between the defined boundaries into one column\n",
    "start = int(startdate.timestamp())\n",
    "stop  = int(enddate.timestamp())\n",
    "df = spark.range(start, stop, 60*60*24).select(col(\"id\").cast(\"timestamp\").cast(\"date\").alias(\"Date\"))\n",
    "\n",
    "# this loops over all rules defined in column_rule_df adding the new columns\n",
    "for row in column_rule_df.collect():\n",
    "    new_column_name = row[\"new_column_name\"]\n",
    "    expression = expr(row[\"expression\"])\n",
    "    df = df.withColumn(new_column_name, expression)\n",
    "\n",
    "df.show(5)\n",
    "# df.orderBy(col(\"Date\").desc()).show(5)\n",
    "# COMMAND ----------\n",
    "df.select(\"Date\", \"IsHoliday\", \"DayOfWeekNameLong_DK\", \"HolidayName\").filter((col(\"IsHoliday\") == 1) & (col(\"Year\") == 2025) & (col(\"DayOfWeek\") != 1) & (col(\"DayOfWeek\") != 7)).orderBy(col(\"Date\")).show(100)\n",
    "df.createOrReplaceTempView(\"dates\")\n",
    "spark.sql(\"SELECT * FROM dates WHERE isToady = 1\").show(20)\n",
    "# display(df.withColumn(\"Playground\", expr(\"date_format(date, 'yyyyMMDD')\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
